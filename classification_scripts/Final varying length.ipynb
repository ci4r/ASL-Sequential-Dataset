{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# Create a MirroredStrateg, If Multi-GPU available\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' \n",
    "tf.config.set_soft_device_placement(True)\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=['/gpu:1','/gpu:2']) \n",
    "# print('Number of GPUs being used: {}'.format(strategy.num_replicas_in_sync))\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import efficientnet.keras as efn\n",
    "import h5py, glob, re, cv2, math, matplotlib, pickle, gc\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import itertools, random\n",
    "from collections import Counter\n",
    "# from cnn_utils import *\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from PIL import Image  \n",
    "import pdb\n",
    "from statistics import mode \n",
    "from IPython.display import clear_output\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "np.random.seed(1)\n",
    "# Setting the seed for python random numbers\n",
    "random.seed(1254)\n",
    "# Setting the graph-level random seed.\n",
    "tf.random.set_seed(89)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "!CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy(devices=['/gpu:1','/gpu:2'])\n",
    "# print('Number of GPUs being used: {}'.format(strategy.num_replicas_in_sync))\n",
    "# print('Number of GPUs being used: {}'.format(strategy.num_replicas_in_sync))\n",
    "# def setup_multi_node_training(): # IMPORTANT: SET UP TF_CONFIG FOR MULTINODE TRAINING HERE os.environ[“TF_FORCE_GPU_ALLOW_GROWTH”] = “true” tf.config.set_soft_device_placement(True) mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.NCCL) # Constructs the configuration run_config = tf.estimator.RunConfig( train_distribute=mirrored_strategy, ) return run_config\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 120, 128, 128, 3)\n",
      "(200, 120, 128, 128, 3)\n",
      "(800, 120, 19)\n",
      "(200, 120, 19)\n",
      "(800,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# filename = 'Datasets/final_imit_spec.pkl'\n",
    "# filename = 'Datasets/final_imit_spec2_128.pkl'\n",
    "filename = 'Datasets/final_imit_spec2_128_last.pkl'\n",
    "with open(filename, 'rb') as input:\n",
    "    x = pickle.load(input)\n",
    "imit_train_images, imit_test_images, imit_y_train, imit_y_test, imit_env_train, imit_env_test = [x[0], x[1]\n",
    "                                                                                                , x[2], x[3]\n",
    "                                                                                                , x[4], x[5]]\n",
    "print(imit_train_images.shape)\n",
    "print(imit_test_images.shape)\n",
    "print(imit_y_train.shape)\n",
    "print(imit_y_test.shape)\n",
    "print(imit_env_train.shape)\n",
    "print(imit_env_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected File: Datasets/final_imit_RD-RA2_128_last.hdf5\n",
      "(800, 605, 128, 128, 3)\n",
      "(800, 605, 128, 128, 3)\n",
      "(200, 605, 128, 128, 3)\n",
      "(200, 605, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "filename = 'Datasets/final_imit_RD-RA2_128_last.hdf5'\n",
    "data = h5py.File(filename, \"r\")\n",
    "print('Selected File: '+str(filename))\n",
    "imit_RD_train = np.array(data[\"train_RD\"])\n",
    "imit_RA_train = np.array(data[\"train_RA\"])\n",
    "imit_RD_test = np.array(data[\"test_RD\"])\n",
    "imit_RA_test = np.array(data[\"test_RA\"])\n",
    "data.close()\n",
    "print(imit_RD_train.shape)\n",
    "print(imit_RA_train.shape)\n",
    "print(imit_RD_test.shape)\n",
    "print(imit_RA_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 120, 5, 128, 128, 3)\n",
      "(200, 120, 5, 128, 128, 3)\n",
      "(800, 120, 5, 128, 128, 3)\n",
      "(200, 120, 5, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# windowed RD\n",
    "interval = range(0,600)\n",
    "x_train11 = np.reshape(imit_RD_train[:,interval,:,:,:], (imit_RD_train.shape[0],120,5,imit_RD_train.shape[2],imit_RD_train.shape[3],imit_RD_train.shape[4]))\n",
    "x_test11 = np.reshape(imit_RD_test[:,interval,:,:,:], (imit_RD_test.shape[0],120,5,imit_RD_test.shape[2],imit_RD_test.shape[3],imit_RD_test.shape[4]))\n",
    "# y_train11 = np.reshape(imit_y_train_RD[:,interval], (imit_y_train_RD.shape[0],120,5))\n",
    "# y_test11 = np.reshape(imit_y_test_RD[:,interval], (imit_y_test_RD.shape[0],120,5))\n",
    "# y_train11 = to_categorical(np.squeeze(stats.mode(y_train11,2)[0]))\n",
    "# y_test11 = to_categorical(np.squeeze(stats.mode(y_test11,2)[0]))\n",
    "print(x_train11.shape)\n",
    "# print(y_train11.shape)\n",
    "print(x_test11.shape)\n",
    "# print(y_test11.shape)  \n",
    "\n",
    "# windowed RA\n",
    "x_train12 = np.reshape(imit_RA_train[:,interval,:,:,:], (imit_RA_train.shape[0],120,5,imit_RA_train.shape[2],imit_RA_train.shape[3],imit_RA_train.shape[4]))\n",
    "x_test12 = np.reshape(imit_RA_test[:,interval,:,:,:], (imit_RA_test.shape[0],120,5,imit_RA_test.shape[2],imit_RA_test.shape[3],imit_RA_test.shape[4]))\n",
    "print(x_train12.shape)\n",
    "print(x_test12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, labels, batch_size=1):              \n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    data is an array  [[[frame1_filename,frame2_filename,…frame16_filename],label1], [[frame1_filename,frame2_filename,…frame16_filename],label2],……….].\n",
    "    \"\"\"\n",
    "    num_samples = data.shape[0]\n",
    "    \n",
    "    while True:   \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "    #             print ('starting index: ', offset) \n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = data[offset:offset+batch_size]\n",
    "            label = labels[offset:offset+batch_size]\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            # For each example\n",
    "            for i in range(0,batch_samples.shape[0]):\n",
    "                X_train.append(batch_samples[i])\n",
    "                y_train.append(label[i])\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            #X_train = np.rollaxis(X_train,1,4)\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            # yield the next training batch            \n",
    "            yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdcnn2d_bilstm_seq():\n",
    "    with tf.device('/gpu:2'):\n",
    "#     with strategy.scope():\n",
    "        inputlayer1 = Input(shape = (None,128,128,3))\n",
    "        x = TimeDistributed(Conv2D(8, kernel_size=(3, 3), padding='same', activation='relu'))(inputlayer1)\n",
    "        x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = TimeDistributed(Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu'))(x)\n",
    "        x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = TimeDistributed(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))(x)\n",
    "        x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = TimeDistributed(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))(x)\n",
    "        x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "#         x = TimeDistributed(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'))(inputlayer1)\n",
    "#         x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "# #         x = BatchNormalization()(x)\n",
    "#         x = TimeDistributed(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))(x)\n",
    "#         x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "# #         x = BatchNormalization()(x)\n",
    "#         x = TimeDistributed(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))(x)\n",
    "#         x = TimeDistributed(MaxPooling2D(pool_size=(2,2)))(x)\n",
    "#         x = TimeDistributed(MaxPooling2D(pool_size=(4,1)))(x)\n",
    "#         x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "#         x = Reshape(target_shape = (-1,64*8*8))(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "#         x = Bidirectional(LSTM(8, dropout=0.5, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True))(x)\n",
    "#         x = Dropout(0.5)(x)\n",
    "        out1 = Dense(num_class,activation='softmax')(x)\n",
    "#         out1 = TimeDistributed(Dense(num_class,activation='softmax'))(x, mask=maskinput)\n",
    "#         model = keras.Model(inputs = [inputlayer1,maskinput], outputs = [out1])#, out2, out3])\n",
    "        model = keras.Model(inputs = [inputlayer1], outputs = [out1])#, out2, out3])\n",
    "        opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer=opt,metrics = ['accuracy'])\n",
    "        return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdcnn3d_bilstm_seq():\n",
    "    with tf.device('/gpu:2'):\n",
    "#     with strategy.scope():\n",
    "        inputlayer1 = Input(shape = (None,5,128,128,3))\n",
    "        x = TimeDistributed(Conv3D(8, kernel_size=(3, 3, 3), padding='same', activation='relu'))(inputlayer1)\n",
    "        x = TimeDistributed(MaxPooling3D(pool_size=(1, 2, 2)))(x)\n",
    "        x = TimeDistributed(Conv3D(16, kernel_size=(3, 3, 3), padding='same', activation='relu'))(x)\n",
    "        x = TimeDistributed(MaxPooling3D(pool_size=(1, 2, 2)))(x)\n",
    "        x = TimeDistributed(Conv3D(32, kernel_size=(3, 3, 3), padding='same', activation='relu'))(x)\n",
    "        x = TimeDistributed(MaxPooling3D(pool_size=(1, 2, 2)))(x)\n",
    "        x = TimeDistributed(Conv3D(64, kernel_size=(3, 3, 3), padding='same', activation='relu'))(x)\n",
    "        x = TimeDistributed(MaxPooling3D(pool_size=(1, 2, 2)))(x)\n",
    "        x = TimeDistributed(MaxPooling3D(pool_size=(1, 4, 1)))(x)\n",
    "        x = TimeDistributed(Flatten())(x)\n",
    "#         x = Reshape(target_shape = (-1, 64*8*8*5))(x)\n",
    "        x = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True))(x)\n",
    "        \n",
    "        out1 = Dense(num_class,activation='softmax')(x)\n",
    "#         out1 = TimeDistributed(Dense(num_class,activation='softmax'))(x, mask=maskinput)\n",
    "#         model = keras.Model(inputs = [inputlayer1,maskinput], outputs = [out1])#, out2, out3])\n",
    "        model = keras.Model(inputs = [inputlayer1], outputs = [out1])#, out2, out3])\n",
    "        opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer=opt,metrics = ['accuracy'])\n",
    "        return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = 10\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 sec models...\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.6415 - accuracy: 0.8605\n",
      "md acc: 0.8604999780654907\n",
      "200/200 [==============================] - 9s 46ms/step - loss: 0.2231 - accuracy: 0.9240\n",
      "rd acc: 0.9239583611488342\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.3501 - accuracy: 0.8972\n",
      "ra acc: 0.8971666693687439\n",
      "Training 2 sec models...\n",
      "400/400 [==============================] - 7s 17ms/step - loss: 0.8797 - accuracy: 0.8431\n",
      "md acc: 0.8430833220481873\n",
      "400/400 [==============================] - 10s 24ms/step - loss: 0.4789 - accuracy: 0.9003\n",
      "rd acc: 0.9002916812896729\n",
      "400/400 [==============================] - 10s 25ms/step - loss: 0.3999 - accuracy: 0.8593\n",
      "ra acc: 0.859333336353302\n",
      "Training 3 sec models...\n",
      "600/600 [==============================] - 6s 11ms/step - loss: 0.6568 - accuracy: 0.8456\n",
      "md acc: 0.8455833196640015\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.5064 - accuracy: 0.8991\n",
      "rd acc: 0.8991249799728394\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.5976 - accuracy: 0.8700\n",
      "ra acc: 0.8700000047683716\n",
      "Training 6 sec models...\n",
      "1200/1200 [==============================] - 8s 7ms/step - loss: 0.7596 - accuracy: 0.8126\n",
      "md acc: 0.812583327293396\n",
      "1200/1200 [==============================] - 13s 11ms/step - loss: 0.5927 - accuracy: 0.8235\n",
      "rd acc: 0.8234583139419556\n",
      "1200/1200 [==============================] - 13s 11ms/step - loss: 0.6516 - accuracy: 0.7899\n",
      "ra acc: 0.7899166941642761\n",
      "Training 12 sec models...\n",
      "2400/2400 [==============================] - 14s 6ms/step - loss: 0.9564 - accuracy: 0.7857\n",
      "md acc: 0.7857083082199097\n",
      "2400/2400 [==============================] - 16s 7ms/step - loss: 0.7956 - accuracy: 0.7631\n",
      "rd acc: 0.7631250023841858\n",
      "2400/2400 [==============================] - 17s 7ms/step - loss: 1.1053 - accuracy: 0.7373\n",
      "ra acc: 0.737333357334137\n",
      "Training 24 sec models...\n",
      "4800/4800 [==============================] - 23s 5ms/step - loss: 1.1113 - accuracy: 0.6916\n",
      "md acc: 0.6915833353996277\n",
      "4800/4800 [==============================] - 30s 6ms/step - loss: 0.9609 - accuracy: 0.7247\n",
      "rd acc: 0.7247499823570251\n",
      "4800/4800 [==============================] - 29s 6ms/step - loss: 1.0394 - accuracy: 0.6985\n",
      "ra acc: 0.6984583139419556\n"
     ]
    }
   ],
   "source": [
    "seqlen = [1, 2, 3, 6, 12, 24]\n",
    "num_class = imit_y_train.shape[-1]\n",
    "for i in range(len(seqlen)):\n",
    "    xtr_md = imit_train_images.reshape(-1,120//seqlen[i],128,128,3)\n",
    "    xts_md = imit_test_images.reshape(-1,120//seqlen[i],128,128,3)\n",
    "    ytr = imit_y_train.reshape(-1,120//seqlen[i],19)\n",
    "    yts = imit_y_test.reshape(-1,120//seqlen[i],19)\n",
    "    xtr_rd = x_train11.reshape(-1,120//seqlen[i],5,128,128,3)\n",
    "    xts_rd = x_test11.reshape(-1,120//seqlen[i],5,128,128,3)\n",
    "    xtr_ra = x_train12.reshape(-1,120//seqlen[i],5,128,128,3)\n",
    "    xts_ra = x_test12.reshape(-1,120//seqlen[i],5,128,128,3)\n",
    "    print('Training '+str(seqlen[i])+' sec models...')\n",
    "    \n",
    "    # md\n",
    "    model = tdcnn2d_bilstm_seq()\n",
    "    train_dataset = data_generator(xtr_md, ytr)\n",
    "    validation_dataset = data_generator(xts_md, yts)\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        batch_size = 32,\n",
    "        epochs=100,\n",
    "        steps_per_epoch = len(ytr), validation_steps = len(yts),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose = 0\n",
    "    )\n",
    "    loss, acc = model.evaluate(validation_dataset, steps = len(yts))\n",
    "    print('md acc: '+str(acc))\n",
    "    fname = 'final varying md len_'+str(seqlen[i])\n",
    "    MODEL_FILE = 'Models/' + fname + '.json' # save path\n",
    "    WEIGHT_FILE = 'Models/' + fname + '.h5' # save path\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(MODEL_FILE, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(WEIGHT_FILE)\n",
    "    \n",
    "    # rd\n",
    "    model = tdcnn3d_bilstm_seq()\n",
    "    train_dataset = data_generator(xtr_rd, ytr)\n",
    "    validation_dataset = data_generator(xts_rd, yts)\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        batch_size = 32,\n",
    "        epochs=100,\n",
    "        steps_per_epoch = len(ytr), validation_steps = len(yts),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose = 0\n",
    "    )\n",
    "    loss, acc = model.evaluate(validation_dataset, steps = len(yts))\n",
    "    print('rd acc: '+str(acc))\n",
    "    fname = 'final varying rd len_'+str(seqlen[i])\n",
    "    MODEL_FILE = 'Models/' + fname + '.json' # save path\n",
    "    WEIGHT_FILE = 'Models/' + fname + '.h5' # save path\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(MODEL_FILE, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(WEIGHT_FILE)\n",
    "    \n",
    "    # ra\n",
    "    model = tdcnn3d_bilstm_seq()\n",
    "    train_dataset = data_generator(xtr_ra, ytr)\n",
    "    validation_dataset = data_generator(xts_ra, yts)\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        batch_size = 32,\n",
    "        epochs=100,\n",
    "        steps_per_epoch = len(ytr), validation_steps = len(yts),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose = 0\n",
    "    )\n",
    "    loss, acc = model.evaluate(validation_dataset, steps = len(yts))\n",
    "    print('ra acc: '+str(acc))\n",
    "    fname = 'final varying ra len_'+str(seqlen[i])\n",
    "    MODEL_FILE = 'Models/' + fname + '.json' # save path\n",
    "    WEIGHT_FILE = 'Models/' + fname + '.h5' # save path\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(MODEL_FILE, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(WEIGHT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sec prints are reverse !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(fname):\n",
    "    model_file = 'Models/' + fname + '.json'\n",
    "    w_file = 'Models/' + fname + '.h5'\n",
    "    json_file = open(model_file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model_md = model_from_json(loaded_model_json)\n",
    "    model_md.load_weights(w_file)\n",
    "    return model_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = [1, 2, 3, 6, 12, 24]\n",
    "mod_md_1sec = loadmodel('final varying md len_'+str(seqlen[-1]))\n",
    "mod_md_2sec = loadmodel('final varying md len_'+str(seqlen[-2]))\n",
    "mod_rd_1sec = loadmodel('final varying rd len_'+str(seqlen[-1]))\n",
    "mod_rd_2sec = loadmodel('final varying rd len_'+str(seqlen[-2]))\n",
    "mod_ra_1sec = loadmodel('final varying ra len_'+str(seqlen[-1]))\n",
    "mod_ra_2sec = loadmodel('final varying ra len_'+str(seqlen[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [mod_md_1sec,mod_md_2sec,mod_rd_1sec,mod_rd_2sec,mod_ra_1sec,mod_ra_2sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xts_md = imit_test_images.reshape(-1,120//seqlen[-1],128,128,3)\n",
    "t = time.time()\n",
    "p = mod_md_1sec.predict(xts_md,batch_size=1,verbose=0)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.318246603012085"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xts_md = imit_test_images.reshape(-1,120//seqlen[-2],128,128,3)\n",
    "t = time.time()\n",
    "p = mod_md_2sec.predict(xts_md,batch_size=512,verbose=0)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.793370246887207"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xts_rd = x_test11.reshape(-1,120//seqlen[-1],5,128,128,3)\n",
    "t = time.time()\n",
    "p = mod_rd_1sec.predict(xts_rd,batch_size=128,verbose=0)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.334917783737183"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xts_rd = x_test11.reshape(-1,120//seqlen[-2],5,128,128,3)\n",
    "t = time.time()\n",
    "p = mod_rd_2sec.predict(xts_rd,batch_size=128,verbose=0)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.35310673713684"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xts_ra = x_test12.reshape(-1,120//seqlen[-1],5,128,128,3)\n",
    "t = time.time()\n",
    "p = mod_ra_1sec.predict(xts_ra,batch_size=128,verbose=0)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.054130792617798"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xts_ra = x_test12.reshape(-1,120//seqlen[-2],5,128,128,3)\n",
    "t = time.time()\n",
    "p = mod_ra_2sec.predict(xts_ra,batch_size=128,verbose=0)\n",
    "time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
